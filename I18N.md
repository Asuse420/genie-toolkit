# Internationalization

We are extending Genie to support multiple languages and currently are working on Chinese. If you'd like to contribute the internationalization of Genie, please read the following instructions.

## Construct Template

## Dataset Translation

To add a new language, the dataset of the target language is needed. We demonstrate how to build a Traditional Chinese dataset from the English one.

#### Step 1. Download a source ThingTalk dataset.

Download the dataset from Thingpedia. It contains the primitive sentences of all devices uploaded on the Thingpedia. Datasets of other languages will be available in the future.

```bash
genie download-dataset -o dataset.en-US.tt
```

#### Step 2. Clean the dataset.

We recommend you clean the dataset before translating. The downloaded dataset includes not only utterances to translate but also `preprocessed`, `id`, `click_count`, and `like_count`. You will definitely feel better if you don't see these extra information when translating the utterances.

```bash
genie dataset -i dataset.en-US.tt -o dataset.raw.en-US.tt -l zh-tw --thingpedia thingpedia.json --actions clean
```

Modify the language tag in the first line of the dataset. Note that the language tag here does not equal to the locale. E.g. English is "en"; Chinese is "zh"; etc. The language tag will be recognized by the Almond tokenizer later.

```javascript
dataset @org.thingpedia.dynamic.everything language "zh" {
  ...
}
```

Rename the dataset file as `dataset.raw.zh-tw.tt`.

#### Step 3. Translate all utterances in the dataset manually.

This part requires a lot of hard work. You can translate them all yourself or you might want to hire some part-time workers to do it.

#### Step 4. Preprocess the translated dataset.

The last step is to preprocess all translated sentences before Genie sentence synthesis. We unify the letter case in this action. And for languages such as Chinese, Japanese, and Korean, sentences are segmented into words by the Almond tokenizer. It adds the `preprocessed` field, which is essential for generating synthesized sentences, to each command in the translated dataset.

The Stanford Almond Tokenizing Service recognize only English for the time being. So you need to setup a tokenizer locally. See [here](https://github.com/stanford-oval/genie-toolkit#step-0-optional-setup). Then set the environmental variable `GENIE_USE_TOKENIZER` to be `local`.

```bash
export GENIE_USE_TOKENIZER=local
```

After the Almond tokenizer listening on http://localhost:8888, you can preprocess the sentences.

```bash
genie dataset -i dataset.raw.zh-tw.tt -o dataset.zh-tw.tt -l zh-tw --thingpedia thingpedia.json --actions preprocess
```

The Almond tokenizer does not support Traditional Chinese and perform poorly on it. So our workaround is to convert sentences from Tranditional Chinese to Simplified Chinese first, segment them, and convert them back.

#### Step 5. Synthesize sentences with Genie.

Then you can start to synthesize sentences from your translated dataset!

```bash
genie generate --locale zh-tw --template languages/zh-tw/thingtalk.genie --thingpedia thingpedia.json --dataset dataset.zh-tw.tt -o synthetic.zh-tw.tsv
```